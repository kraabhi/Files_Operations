######   Accuracy for Backend   #############################################################################################

rm(list=ls())

#setwd(commandArgs(TRUE)[1])

solution <- read.csv(file=commandArgs(TRUE)[2], stringsAsFactors=TRUE,header=TRUE)
submission<- read.csv(file=commandArgs(TRUE)[3], stringsAsFactors=TRUE,header=TRUE)

mer_all <-merge(x = solution, y = submission, by = "filename", all.x = TRUE)

public<- mer_all[ which(mer_all$Label=='public'), ]
private<- mer_all[ which(mer_all$Label=='private'), ]

private_score <- sum(ifelse(private[,2] == private[,4],1,0))/nrow(private)
write.csv(private_score, commandArgs(TRUE)[4], row.names=FALSE)

public_score <- sum(ifelse(public[,2] == public[,4],1,0))/nrow(public)
write.csv(public_score, commandArgs(TRUE)[5], row.names=FALSE)


######   Accuracy in local   ####################################################################################################
rm(list=ls())

#setwd(commandArgs(TRUE)[1])

solution <- read.csv(file="C:/Users/admin/Documents/Cax/Contest_characters/SolutionFile_characters.csv", stringsAsFactors=TRUE,header=TRUE)
submission<- read.csv(file="C:/Users/admin/Desktop/Book2.csv", stringsAsFactors=TRUE,header=TRUE)

mer_all <-merge(x = solution, y = submission, by = "Filename", all.x = TRUE)

public<- mer_all[ which(mer_all$Label=='public'), ]
private<- mer_all[ which(mer_all$Label=='private'), ]

private_score <- sum(ifelse(private[,2] == private[,4],1,0))/nrow(private)
#write.csv(private_score, "C:/Users/admin/Documents/Cax/Contest_characters/file.csv", row.names=FALSE)

public_score <- sum(ifelse(public[,2] == public[,4],1,0))/nrow(public)
#write.csv(public_score, "C:/Users/admin/Documents/Cax/Contest_characters/file_public.csv", row.names=FALSE)

################### F1 score #########################################################################################
#install.packages('caret', dependencies = TRUE)
#library(MLmetrics)
library(caret)
solution <- read.csv(file="C:/Users/admin/Documents/Cax/contest_mortage/Contest_Mortgage/CAX_SolutionFile.csv", stringsAsFactors=TRUE,header=TRUE)
submission<- read.csv(file="C:/Users/admin/Desktop/practice_problems/mortgage_new/1_not_funded.csv",stringsAsFactors = TRUE , header = TRUE)

solution <- read.csv(file=commandArgs(TRUE)[2], stringsAsFactors=TRUE,header=TRUE)
submission<- read.csv(file=commandArgs(TRUE)[3], stringsAsFactors=TRUE,header=TRUE)

mer_all <-merge(x = solution, y = submission, by = "Unique_ID", all.x = TRUE)

public<- mer_all[ which(mer_all$Label=='public'), ]
private<- mer_all[ which(mer_all$Label=='private'), ]

###################calculating Fscore for public leader board #######################################################################
y2 = public[,2]
predictions  = public[,4]

accuracy = Accuracy(predictions, y2)
ConfusionMatrix(predictions, y2)
public_score = F1_Score(predictions, y2, positive = "NOT FUNDED")

write.csv(public_score, commandArgs(TRUE)[5], row.names=FALSE)

###################calculating Fscore for private leader board ######################################################################

y1 = private[,2]
predictions1  = private[,4]

accuracy = Accuracy(predictions1, y1)
ConfusionMatrix(predictions1, y1)
private_score = F1_Score(predictions1, y1, positive = NULL)

write.csv(private_score, commandArgs(TRUE)[4], row.names=FALSE)

################# Macro F1 Score Backend #############################################################################################

import pandas as pd
import numpy as np
import sys
from sklearn.metrics import f1_score

# first_arg =pd.read_csv("C:/Users/admin/Documents/Cax/contest_mortage/Contest_Mortgage/CAX_SolutionFile.csv")
# second_arg =pd.read_csv("C:/Users/admin/Desktop/practice_problems/mortgage_new/full.csv")

first_arg =pd.read_csv(sys.arg[1])
second_arg =pd.read_csv(sys.arg[2])

mer_all =pd.merge(first_arg, second_arg, on = "Unique_ID")
mer_all.head()

mer_all['Result_Predicted_x']=mer_all['Result_Predicted_x'].map({'NOT FUNDED': 0,'FUNDED':1} ) 
mer_all['Result_Predicted_y']=mer_all['Result_Predicted_y'].map({'NOT FUNDED': 0,'FUNDED':1} ) 

public = mer_all[mer_all["Label"]=="public"]
private = mer_all[mer_all["Label"]=="private"]

# public F1 score

public_predicted = public["Result_Predicted_y"]
public_actual = public["Result_Predicted_x"]
pb_score = f1_score(public_actual ,public_predicted,average='macro')
public_score =pd.DataFrame({'x' : [pb_score]})

#public_score.to_csv('pub_score.csv',index = False)
public_score.to_csv(sys.argv[4],index = False)
#public_score

# private F1 score

private_predicted = private["Result_Predicted_y"]
private_actual = private["Result_Predicted_x"]
pri_score = f1_score(private_actual ,private_predicted,average='macro')
private_score =pd.DataFrame({'x' : [pri_score]})

#private_score.to_csv('pri_score.csv',index = False)
private_score.to_csv(sys.argv[3],index = False)
#private_score
